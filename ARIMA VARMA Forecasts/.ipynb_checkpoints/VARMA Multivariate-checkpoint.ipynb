{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e463c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATED FROM COMBINING RESEARCH FROM:\n",
    "\n",
    "https://analyticsindiamag.com/a-guide-to-varma-with-auto-arima-in-time-series-modelling/\n",
    "https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/\n",
    "\n",
    "and credits for how to create a VARMA model as well\n",
    "\n",
    "https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import plot_importance\n",
    "from pmdarima import auto_arima\n",
    "from sklearn import metrics\n",
    "\n",
    "#Imported to ignore warnings from ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#14 import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce17f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#3 lines written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Easy data format and read\n",
    "\"\"\"\n",
    "\n",
    "def format_data(path):\n",
    "    df = pd.read_excel(path, index_col='Year', parse_dates=True)\n",
    "    df2 = df.drop(columns=['Country Name', 'Country Code'])\n",
    "    df2 = df2.dropna(axis=1, how='all')\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filling logic\n",
    "\"\"\"\n",
    "def fillmissing(data):\n",
    "        \n",
    "    newdata = data.fillna(data.mean())\n",
    "    \n",
    "    return newdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "adapted, modified and inspired from:\n",
    "\n",
    "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n",
    "\n",
    "modified structure to fit my dataset\n",
    "\"\"\"\n",
    "\n",
    "def feature_selection(model, X_train, X_test, y_train):\n",
    "    \n",
    "    selector = SelectFromModel(model, max_features=3, threshold=-np.inf)\n",
    "    selector.fit (X_train, y_train)\n",
    "    \n",
    "    select_X_train = selector.transform(X_train)\n",
    "    select_X_test = selector.transform(X_test)\n",
    "    \n",
    "    #print (selector.get_feature_names_out())\n",
    "    \n",
    "    selection_model = XGBRegressor(objective='reg:squarederror', n_estimators=1000, max_depth=1, learning_rate = 0.1)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    \n",
    "    select_y_pred = selection_model.predict(select_X_test)\n",
    "    \n",
    "    #Line 24 modified from:\n",
    "    #https://stackoverflow.com/questions/54933804/how-to-get-actual-feature-names-in-xgboost-feature-importance-plot-without-retra\n",
    "    selection_model.get_booster().feature_names = selector.get_feature_names_out().tolist()\n",
    "    plot_importance(selection_model.get_booster())\n",
    "    \n",
    "    return selector ,select_X_train ,select_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IDEA OF USING AUTO-ARIMA IN CONJUCTION WITH VARMA GOES TO:\n",
    "\n",
    "https://analyticsindiamag.com/a-guide-to-varma-with-auto-arima-in-time-series-modelling/\n",
    "\"\"\"\n",
    "\n",
    "def stepwisefits(data, name=\"\"):\n",
    "    stepwise_fit = auto_arima(data, trace=True, suppress_warnings=True)\n",
    "    \n",
    "    print(f'{name} best')\n",
    "    stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "line 9,10,15 (MDA) modified from: https://gist.github.com/bshishov/5dc237f59f019b26145648e2124ca1c9\n",
    "\n",
    "line 12 (MAPE) adapted from: https://www.statology.org/mape-python/\n",
    "\"\"\"\n",
    "\n",
    "def performance_metrics(y_test, y_pred):\n",
    "    \n",
    "    sign1 = np.sign(np.array(y_test[1:]) - np.array(y_test[:-1]))\n",
    "    sign2 = np.sign(np.array(y_pred[1:]) - np.array(y_pred[:-1]))\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test )) *100\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    mda = np.mean((sign1 == sign2).astype(int))\n",
    "    mean = np.mean(y_test)\n",
    "    si = (rmse/mean)*100\n",
    "    \n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"MAPE: \", mape)\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"Scatter Index: \", si)\n",
    "    print(\"MDA: \", mda)\n",
    "    print(\"Mean of actual: \", mean)\n",
    "    \n",
    "#16 lines total, 9 lines me, 3 lines modified, 3 lines documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = format_data('/Users/farhanhabibie/Desktop/Farhan Thesis Code /UG-Project-Farhan/Multivariate More.xlsx')\n",
    "data.drop(data.tail(1).index,inplace=True) #remove last row\n",
    "filled_data = fillmissing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Written by me but uses function calls to pandas and sklearn\n",
    "\"\"\"\n",
    "\n",
    "X = filled_data.drop(columns=['GDP growth (annual %)'])\n",
    "y = filled_data['GDP growth (annual %)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c72dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Written by me but uses function calls to xgb library\n",
    "\"\"\"\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=1000, max_depth=1, learning_rate=0.2)\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "#2 lines from documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector, select_X_train, select_y_pred = feature_selection(model, X_train, X_test, y_train)\n",
    "\n",
    "#1 line written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_indicators = filled_data[['GDP growth (annual %)', \n",
    "                               'General government final consumption expenditure (% of GDP)',\n",
    "                              'Final consumption expenditure (% of GDP)',\n",
    "                              'Foreign direct investment, net inflows (% of GDP)',\n",
    "                              'Foreign direct investment, net outflows (% of GDP)',\n",
    "                              'Exports of goods and services (% of GDP)', \n",
    "                               'Imports of goods and services (% of GDP)']]\n",
    "\n",
    "#1 line written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the plot above, select columns for multivariate forecast and join with gdp growth\n",
    "featurenames = selector.get_feature_names_out().tolist()\n",
    "selected_data = filled_data[featurenames]\n",
    "gdp_data = pd.DataFrame(y)\n",
    "joined_data = pd.concat([selected_data, gdp_data], axis=1)\n",
    "\n",
    "#5 lines written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Method for this way of checking for stationarity comes from and refactored from the guide:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/\n",
    "\"\"\"\n",
    "#Check stationarity\n",
    "coint_johansen(best_indicators, -1,1).eig\n",
    "\n",
    "#1 line from guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train test\n",
    "\"\"\"\n",
    "adapted and modified from: \n",
    "https://towardsdatascience.com/time-series-from-scratch-train-test-splits-and-evaluation-metrics-4fd654de1b37\n",
    "\"\"\"\n",
    "train = best_indicators[:int(len(best_indicators)*0.8)]\n",
    "test = best_indicators[int(len(best_indicators)*0.8):]\n",
    "\n",
    "#3 lines from guide but modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6db64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, column in train.iteritems():\n",
    "    stepwisefits(column, name=column.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f563efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Just using function calls to the VARMA library \n",
    "\"\"\"\n",
    "\n",
    "model_V = VARMAX(train, order=(1,0))\n",
    "model_Vf = model_V.fit(disp=False)\n",
    "y_pred = model_Vf.predict(start=len(train), end=len(train)+len(test)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a25918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(y_pred)\n",
    "df2 = df_pred['GDP growth (annual %)']\n",
    "test_gdp = test['GDP growth (annual %)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ca4fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code written by me, however (line 9) was also used in a previous Introduction to AI Course I partook in,\n",
    "at City University of London\n",
    "\n",
    "Link to repository provided: \n",
    "https://github.com/LabiKSV/intro-to-ai-farhan-labi/blob/main/Linear%20Regression%20Label%20Encoder.ipynb\n",
    "\"\"\"\n",
    "\n",
    "df_compare = pd.DataFrame({'Actual' : test_gdp, 'Predicted' : df2})\n",
    "df_compare.plot(title='GDP Growth Actual vs Predicted')\n",
    "performance_metrics(test_gdp, df2)\n",
    "\n",
    "#3 lines written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"86 Lines of code\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
