{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from pyGRNN import GRNN\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51078c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#3 lines written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that loads and prepares the dataframe. \n",
    "\n",
    "Converts the data into time-series readable.\n",
    "\"\"\"\n",
    "def initialFormat (filepath, indicatorcode):\n",
    "    #Reads the file and creates a dataframe from it\n",
    "    df = pd.read_excel(filepath)\n",
    "    \n",
    "    #Choose what to forecast using indicator code\n",
    "    df_icode = df.loc[df['Indicator Code'] == indicatorcode]\n",
    "    \n",
    "    #Dropping these columns as they are not needed for the forecast\n",
    "    df_icode = df_icode.drop(columns=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'])\n",
    "    \n",
    "    #Swap axis so it is in the proper format\n",
    "    df_formatted = df_icode.swapaxes(\"index\", \"columns\")\n",
    "    \n",
    "    #Renaming column name to 'values' to make reference easier\n",
    "    for col_names in df_formatted.columns:\n",
    "        name = df_formatted.rename(columns={col_names : \"Val\"})\n",
    "        return name\n",
    "    \n",
    "    return df_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refactored from: \n",
    "\n",
    "https://cprosenjit.medium.com/10-time-series-forecasting-methods-we-should-know-291037d2e285\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def shift_dataframe(data):\n",
    "    \n",
    "    data[\"Target\"] = data.Val.shift(-1)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#4 lines taken from guide above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a833374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_imputation(data):\n",
    "    filled = data.fillna(data.mean())\n",
    "    filled.drop(data.tail(1).index,inplace=True) #remove last row\n",
    "    \n",
    "    return filled\n",
    "\n",
    "#3 lines written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa872a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "referenced from: https://towardsdatascience.com/time-series-from-scratch-train-test-splits-and-evaluation-metrics-4fd654de1b3\n",
    "\"\"\"\n",
    "\n",
    "def train_test_split(data):\n",
    "    \n",
    "    train = data[:int(len(data)*0.8)]\n",
    "    test = data[int(len(data) * 0.8):]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y_test, y_pred):\n",
    "    \n",
    "    sign1 = np.sign(np.array(y_test[1:]) - np.array(y_test[:-1]))\n",
    "    sign2 = np.sign(np.array(y_pred[1:]) - np.array(y_pred[:-1]))\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test )) *100\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    mda = np.mean((sign1 == sign2).astype(int))\n",
    "    mean = np.mean(y_test)\n",
    "    si = (rmse/mean)*100\n",
    "    \n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"MAPE: \", mape)\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"Scatter Index: \", si)\n",
    "    print(\"MDA: \", mda)\n",
    "    print(\"Mean of actual: \", mean)\n",
    "    \n",
    "#15 Lines. 3 From documentation, 3 from github, 9 written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e69331",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = initialFormat('/Users/farhanhabibie/Desktop/Farhan Thesis Code /UG-Project-Farhan/Indonesia Macro Dataset.xlsx', \n",
    "                     'NY.GDP.MKTP.KD.ZG')\n",
    "filled = mean_imputation(data)\n",
    "shifted = shift_dataframe(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shifted.Val.values.reshape(-1,1)\n",
    "y = shifted.Target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explanation of TSCV from: \n",
    "\n",
    "https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n",
    "\n",
    "[Produces exactly the same output as WFV from the file 'Univariate Forecast Final XGBoost']\n",
    "Code for TSCV from: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n",
    "\"\"\"\n",
    "\n",
    "tscv = TimeSeriesSplit(test_size=len(filled) - int(len(filled)*0.8), n_splits=4)\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=1000,\n",
    "                    max_depth=6, learning_rate=0.3)\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = data.fillna(data.mean())\n",
    "dtest.drop(data.tail(1).index, inplace=True)\n",
    "newindextest = dtest[int(len(data)*0.8):]\n",
    "newindex = newindextest.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a54f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_compare = pd.DataFrame({'Actual' : y_test, 'Predicted' : y_pred})\n",
    "df_compare.index = newindex\n",
    "df_compare.plot(title= 'Unemployment (M) Actual vs Predicted')\n",
    "performance_metrics(y_test, y_pred)\n",
    "#5 lines written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learning Curve to detect overfit/underfit\n",
    "\n",
    "Method from: \n",
    "\n",
    "https://vitalflux.com/learning-curves-explained-python-sklearn-example/amp/\n",
    "and\n",
    "https://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html#:~:text=Learning%20curves%20can%20be%20generated,see%20in%20the%20following%20examples.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pipeline = make_pipeline(model)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipeline, X=X_train,\n",
    "                                                      y=y_train, cv=5,\n",
    "                                                      train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, marker='o', label='Training Accuracy')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.plot(train_sizes, test_mean, marker='x', linestyle='--', label='Validation Accuracy')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Learning Curve, XGB Unemployment (F)')\n",
    "\n",
    "#14 lines taken directly from guide, integrated with my model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547781a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
